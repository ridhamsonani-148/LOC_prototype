version: 0.2

phases:
  install:
    runtime-versions:
      nodejs: 20
      python: 3.11
    commands:
      - echo "Installing AWS CDK CLI..."
      - npm install -g aws-cdk@latest
      - cd backend
      - npm install

  pre_build:
    commands:
      - echo "Building TypeScript..."
      - npm run build
      - echo "Bootstrapping CDK..."
      - cdk bootstrap --require-approval never

  build:
    commands:
      - |
        if [ "$ACTION" = "destroy" ]; then
          echo "Destroying stack..."
          cdk destroy LOCstack --force \
            --context projectName="$PROJECT_NAME" \
            --context dataBucketName="$DATA_BUCKET_NAME" \
            --context bedrockModelId="$BEDROCK_MODEL_ID"
        else
          echo "========================================="
          echo "Deploying Chronicling America Pipeline"
          echo "========================================="
          
          echo "Deploying CDK stack (without KB)..."
          cdk deploy LOCstack --require-approval never \
            --context projectName="$PROJECT_NAME" \
            --context dataBucketName="$DATA_BUCKET_NAME" \
            --context bedrockModelId="$BEDROCK_MODEL_ID" \
            --outputs-file outputs.json
          
          echo "Extracting outputs..."
          DATA_BUCKET=$(cat outputs.json | jq -r '.LOCstack.DataBucketName // empty')

          KB_ROLE_ARN=$(cat outputs.json | jq -r '.LOCstack.KnowledgeBaseRoleArn // empty')
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          
          echo ""
          echo "========================================="
          echo "Creating Knowledge Base Resources"
          echo "========================================="
          
          # Step 1: Create or Get Neptune Analytics Graph with Vector Search
          echo "Step 1: Checking for existing Neptune Analytics graph..."
          GRAPH_ID=$(aws neptune-graph list-graphs \
            --region "$CDK_DEFAULT_REGION" \
            --query "graphs[?name=='${PROJECT_NAME}-graph'].id | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$GRAPH_ID" ] && [ "$GRAPH_ID" != "None" ] && [ "$GRAPH_ID" != "null" ]; then
            echo "‚úì Found existing graph: $GRAPH_ID"
            
            # Check graph status
            STATUS=$(aws neptune-graph get-graph \
              --graph-identifier "$GRAPH_ID" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'status' --output text 2>/dev/null || echo "ERROR")
            
            if [ "$STATUS" != "AVAILABLE" ]; then
              echo "‚ö† Graph exists but status is: $STATUS. Waiting for it to become available..."
            else
              echo "‚úì Graph is available"
            fi
          else
            echo "Creating new Neptune Analytics graph (16 m-NCUs) with Vector Search..."
            GRAPH_ID=$(aws neptune-graph create-graph \
              --graph-name "${PROJECT_NAME}-graph" \
              --provisioned-memory 16 \
              --no-public-connectivity \
              --vector-search-configuration '{"dimension":1024}' \
              --tags Project="$PROJECT_NAME" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'id' --output text)
            
            if [ -z "$GRAPH_ID" ] || [ "$GRAPH_ID" = "None" ]; then
              echo "‚úó Failed to create Neptune Analytics graph"
              echo "Check if Neptune Analytics is available in region: $CDK_DEFAULT_REGION"
              exit 1
            fi
            
            echo "‚úì Graph created: $GRAPH_ID"
          fi
          
          GRAPH_ARN="arn:aws:neptune-graph:${CDK_DEFAULT_REGION}:${AWS_ACCOUNT_ID}:graph/${GRAPH_ID}"
          
          # Step 2: Wait for graph to be available
          echo "Step 2: Waiting for graph to be available..."
          MAX_WAIT=600
          ELAPSED=0
          while [ $ELAPSED -lt $MAX_WAIT ]; do
            STATUS=$(aws neptune-graph get-graph \
              --graph-identifier "$GRAPH_ID" \
              --region "$CDK_DEFAULT_REGION" \
              --query 'status' --output text 2>/dev/null || echo "ERROR")
            
            echo "  Graph status: $STATUS (waited ${ELAPSED}s)"
            
            if [ "$STATUS" = "AVAILABLE" ]; then
              echo "‚úì Graph is available"
              break
            elif [ "$STATUS" = "FAILED" ] || [ "$STATUS" = "DELETING" ]; then
              echo "‚úó Graph creation failed with status: $STATUS"
              exit 1
            elif [ "$STATUS" = "ERROR" ] || [ -z "$STATUS" ]; then
              echo "‚úó Failed to get graph status"
              exit 1
            fi
            
            sleep 30
            ELAPSED=$((ELAPSED + 30))
          done
          
          if [ $ELAPSED -ge $MAX_WAIT ]; then
            echo "‚úó Timeout waiting for graph to be available"
            exit 1
          fi
          
          # Step 3: Create or Get Knowledge Base
          echo "Step 3: Checking for existing Bedrock Knowledge Base..."
          KB_ID=$(aws bedrock-agent list-knowledge-bases \
            --region "$CDK_DEFAULT_REGION" \
            --query "knowledgeBaseSummaries[?name=='${PROJECT_NAME}-knowledge-base'].knowledgeBaseId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$KB_ID" ] && [ "$KB_ID" != "None" ] && [ "$KB_ID" != "null" ]; then
            echo "‚úì Found existing Knowledge Base: $KB_ID"
          else
            echo "Creating new Bedrock Knowledge Base..."
            KB_ID=$(aws bedrock-agent create-knowledge-base \
              --name "${PROJECT_NAME}-knowledge-base" \
              --role-arn "$KB_ROLE_ARN" \
              --knowledge-base-configuration '{
                "type": "VECTOR",
                "vectorKnowledgeBaseConfiguration": {
                  "embeddingModelArn": "arn:aws:bedrock:'"$CDK_DEFAULT_REGION"'::foundation-model/amazon.titan-embed-text-v2:0"
                }
              }' \
              --storage-configuration '{
                "type": "NEPTUNE_ANALYTICS",
                "neptuneAnalyticsConfiguration": {
                  "graphArn": "'"$GRAPH_ARN"'",
                  "fieldMapping": {
                    "metadataField": "metadata",
                    "textField": "text"
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'knowledgeBase.knowledgeBaseId' --output text)
            
            if [ -z "$KB_ID" ] || [ "$KB_ID" = "None" ]; then
              echo "‚úó Failed to create Knowledge Base"
              exit 1
            fi
            
            echo "‚úì Knowledge Base created: $KB_ID"
          fi
          
          # Step 4: Get Transformation Lambda ARN
          echo "Step 4: Getting Transformation Lambda ARN..."
          TRANSFORMATION_LAMBDA_ARN=$(aws lambda get-function \
            --function-name "${PROJECT_NAME}-kb-transformation" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Configuration.FunctionArn' \
            --output text 2>/dev/null || echo "")
          
          if [ -z "$TRANSFORMATION_LAMBDA_ARN" ] || [ "$TRANSFORMATION_LAMBDA_ARN" = "None" ]; then
            echo "‚ö† Transformation Lambda not found, proceeding without custom transformation"
            TRANSFORMATION_LAMBDA_ARN=""
          else
            echo "‚úì Found Transformation Lambda: $TRANSFORMATION_LAMBDA_ARN"
          fi
          
          # Step 5: Create or Get Data Source with Custom Transformation
          echo "Step 5: Checking for existing Data Source..."
          BUCKET_ARN="arn:aws:s3:::${DATA_BUCKET}"
          
          DS_ID=$(aws bedrock-agent list-data-sources \
            --knowledge-base-id "$KB_ID" \
            --region "$CDK_DEFAULT_REGION" \
            --query "dataSourceSummaries[?name=='${PROJECT_NAME}-s3-datasource'].dataSourceId | [0]" \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$DS_ID" ] && [ "$DS_ID" != "None" ] && [ "$DS_ID" != "null" ]; then
            echo "‚úì Found existing Data Source: $DS_ID"
          else
            echo "Creating new Data Source with Custom Transformation for GraphRAG..."
            
            # Create Data Source without custom transformation (simpler approach)
            echo "Creating Data Source with automatic entity extraction..."
            DS_ID=$(aws bedrock-agent create-data-source \
              --name "${PROJECT_NAME}-s3-datasource" \
              --description "S3 data source for GraphRAG with automatic processing" \
              --knowledge-base-id "$KB_ID" \
              --data-source-configuration '{
                "type": "S3",
                "s3Configuration": {
                  "bucketArn": "'"$BUCKET_ARN"'",
                  "inclusionPrefixes": ["extracted/"],
                  "inclusionPatterns": ["**/*.txt"]
                }
              }' \
              --vector-ingestion-configuration '{
                "chunkingConfiguration": {
                  "chunkingStrategy": "FIXED_SIZE",
                  "fixedSizeChunkingConfiguration": {
                    "maxTokens": 2000,
                    "overlapPercentage": 10
                  }
                },
                "contextEnrichmentConfiguration": {
                  "type": "BEDROCK_FOUNDATION_MODEL",
                  "bedrockFoundationModelConfiguration": {
                    "modelArn": "arn:aws:bedrock:'"$CDK_DEFAULT_REGION"'::foundation-model/anthropic.claude-3-haiku-20240307-v1:0",
                    "enrichmentStrategyConfiguration": {
                      "method": "CHUNK_ENTITY_EXTRACTION"
                    }
                  }
                }
              }' \
              --region "$CDK_DEFAULT_REGION" \
              --query 'dataSource.dataSourceId' --output text)
            
            if [ -z "$DS_ID" ] || [ "$DS_ID" = "None" ]; then
              echo "‚úó Failed to create Data Source"
              exit 1
            fi
            
            echo "‚úì Data Source created: $DS_ID"
          fi
          
          # Step 6: Update Lambda environment variables
          echo "Step 6: Updating Lambda environment variables..."
          
          # Get current fargate-trigger Lambda config to preserve existing values
          FARGATE_ENV=$(aws lambda get-function-configuration \
            --function-name "${PROJECT_NAME}-fargate-trigger" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          # Update fargate-trigger Lambda with KB IDs (using jq to build proper JSON)
          echo "$FARGATE_ENV" | jq \
            --arg kb_id "$KB_ID" \
            --arg ds_id "$DS_ID" \
            --arg bucket "$DATA_BUCKET" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, DATA_SOURCE_ID: $ds_id, BUCKET_NAME: $bucket})}' > /tmp/fargate_env.json
          
          aws lambda update-function-configuration \
            --function-name "${PROJECT_NAME}-fargate-trigger" \
            --environment file:///tmp/fargate_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          # Update kb-sync-trigger Lambda (preserve existing vars)
          KB_SYNC_ENV=$(aws lambda get-function-configuration \
            --function-name "${PROJECT_NAME}-kb-sync-trigger" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$KB_SYNC_ENV" | jq \
            --arg kb_id "$KB_ID" \
            --arg ds_id "$DS_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, DATA_SOURCE_ID: $ds_id})}' > /tmp/kb_sync_env.json
          
          aws lambda update-function-configuration \
            --function-name "${PROJECT_NAME}-kb-sync-trigger" \
            --environment file:///tmp/kb_sync_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          # Update chat-handler Lambda (preserve existing vars)
          CHAT_ENV=$(aws lambda get-function-configuration \
            --function-name "${PROJECT_NAME}-chat-handler" \
            --region "$CDK_DEFAULT_REGION" \
            --query 'Environment.Variables' \
            --output json)
          
          echo "$CHAT_ENV" | jq \
            --arg kb_id "$KB_ID" \
            --arg model_id "$BEDROCK_MODEL_ID" \
            '{Variables: (. + {KNOWLEDGE_BASE_ID: $kb_id, MODEL_ID: $model_id})}' > /tmp/chat_env.json
          
          aws lambda update-function-configuration \
            --function-name "${PROJECT_NAME}-chat-handler" \
            --environment file:///tmp/chat_env.json \
            --region "$CDK_DEFAULT_REGION" >/dev/null
          
          echo "‚úì Lambda environment variables updated (fargate-trigger, kb-sync-trigger, chat-handler)"
          
          echo ""
          echo "========================================="
          echo "‚úì Deployment Complete!"
          echo "========================================="
          echo "Graph ID: $GRAPH_ID"
          echo "Knowledge Base ID: $KB_ID"
          echo "Data Source ID: $DS_ID"
          echo "Data Bucket: $DATA_BUCKET"
          echo "========================================="
          
          # Extract optional outputs (may not exist in all stacks)
          STATE_MACHINE_ARN=$(cat outputs.json | jq -r '.LOCstack.StateMachineArn // empty')
          API_URL=$(cat outputs.json | jq -r '.LOCstack.APIGatewayURL // empty')
          CHAT_ENDPOINT=$(cat outputs.json | jq -r '.LOCstack.ChatEndpoint // empty')
          
          if [ -z "$DATA_BUCKET" ]; then
            echo "‚ùå ERROR: Could not extract Data Bucket from CDK deployment"
            exit 1
          fi
          
          echo "‚úÖ Deployment successful!"
          echo ""
          echo "========================================="
          echo "Deployment Summary"
          echo "========================================="
          echo "Data Bucket: $DATA_BUCKET"
          echo "Graph ID: $GRAPH_ID"
          echo "Knowledge Base ID: $KB_ID"
          echo "Data Source ID: $DS_ID"
          
          if [ -n "$STATE_MACHINE_ARN" ]; then
            echo "State Machine ARN: $STATE_MACHINE_ARN"
          fi
          
          if [ -n "$API_URL" ]; then
            echo "API Gateway URL: $API_URL"
          fi
          
          if [ -n "$CHAT_ENDPOINT" ]; then
            echo "Chat Endpoint: $CHAT_ENDPOINT"
          fi
          echo ""
          echo "========================================="
          echo "Next Steps"
          echo "========================================="
          
          if [ -n "$STATE_MACHINE_ARN" ]; then
            echo "1. Start pipeline execution:"
            echo "   aws stepfunctions start-execution \\"
            echo "     --state-machine-arn $STATE_MACHINE_ARN \\"
            echo "     --input '{\"start_date\":\"1815-08-01\",\"end_date\":\"1815-08-31\",\"max_pages\":10}'"
            echo ""
          fi
          
          if [ -n "$CHAT_ENDPOINT" ]; then
            echo "2. Test chat API:"
            echo "   curl -X POST $CHAT_ENDPOINT \\"
            echo "     -H 'Content-Type: application/json' \\"
            echo "     -d '{\"question\":\"Who are the people mentioned?\"}'"
            echo ""
          fi
          
          echo "3. Upload documents to S3:"
          echo "   aws s3 cp document.txt s3://${DATA_BUCKET}/extracted/"
          echo ""
          echo "4. Trigger Knowledge Base sync:"
          echo "   aws bedrock-agent start-ingestion-job \\"
          echo "     --knowledge-base-id $KB_ID \\"
          echo "     --data-source-id $DS_ID \\"
          echo "     --region $CDK_DEFAULT_REGION"
          echo ""
          echo "5. Monitor resources:"
          echo "   - Knowledge Base: https://console.aws.amazon.com/bedrock/home?region=${CDK_DEFAULT_REGION}#/knowledge-bases"
          echo "   - Neptune Analytics: https://console.aws.amazon.com/neptune-analytics/home?region=${CDK_DEFAULT_REGION}"
          echo "   - S3 Bucket: https://s3.console.aws.amazon.com/s3/buckets/${DATA_BUCKET}"
          echo ""
        fi

  post_build:
    commands:
      - echo "========================================="
      - echo "Deployment Complete"
      - echo "========================================="
      - |
        if [ "$ACTION" = "deploy" ]; then
          echo "‚úÖ Pipeline deployed successfully"
          echo "üìä Check CloudWatch Logs for Lambda execution details"
          echo "üîç Query Neptune via the chat API endpoint"
        else
          echo "‚úÖ Stack destroyed successfully"
        fi

artifacts:
  files:
    - "**/*"
  base-directory: "backend/cdk.out"
